{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":770223,"sourceType":"datasetVersion","datasetId":401454}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"ee1c4cc0","cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport torchaudio\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport shutil\nimport time\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import random_split, Subset","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T06:21:29.521583Z","iopub.execute_input":"2025-06-28T06:21:29.521816Z","iopub.status.idle":"2025-06-28T06:21:35.085574Z","shell.execute_reply.started":"2025-06-28T06:21:29.521794Z","shell.execute_reply":"2025-06-28T06:21:35.085060Z"}},"outputs":[],"execution_count":1},{"id":"3646b351","cell_type":"code","source":"DATA_DIR = '/kaggle/input/gtzan-genre-collection/genres' \nSAMPLE_RATE = 22050 # GTZAN default sample rate\nDURATION_SECONDS = 30 # Each audio file is 30 seconds\nTARGET_SAMPLES = SAMPLE_RATE * DURATION_SECONDS # Total samples per audio file\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Parameters for Mel Spectrogram\nN_MELS = 128\nN_FFT = 2048\nHOP_LENGTH = 512\n\ndef log(message , file_path=\"log.txt\"):\n    try:\n        with open(file_path,'a') as fl:\n            fl.write(message + '\\n')\n    except Exception as e:\n        print(\"File not found\")\n\n# --- Custom Dataset Class ---\nclass Data_Preprocessing(Dataset):\n    def __init__(self, data_dir, sample_rate=22050, duration_seconds=30, transform=None):\n        self.data_dir = data_dir\n        self.sample_rate = sample_rate\n        self.duration_samples = sample_rate * duration_seconds\n        self.transform = transform\n        self.audio_files = []\n        self.labels = []\n        self.label_map = {}\n        self._load_dataset()\n\n    def _load_dataset(self):\n        genres = [d for d in os.listdir(self.data_dir) if os.path.isdir(os.path.join(self.data_dir, d))]\n        genres.sort()\n        for i, genre in enumerate(genres):\n            self.label_map[genre] = i\n            genre_path = os.path.join(self.data_dir, genre)\n            for audio_file in os.listdir(genre_path):\n                if audio_file.endswith('.au'):\n                    self.audio_files.append(os.path.join(genre_path, audio_file))\n                    self.labels.append(self.label_map[genre])\n\n    def __len__(self):\n        return len(self.audio_files)\n\n    def __getitem__(self, idx):\n        audio_path = self.audio_files[idx]\n        label = self.labels[idx]\n\n        # Load audio (waveform, sample_rate)\n        # normalized to [-1, 1]\n        waveform, sr = torchaudio.load(audio_path)\n        waveform = waveform / waveform.abs().max()\n\n        # Preprocessing: Resampling, Padding/Trimming, and moving to GPU\n        waveform = waveform.to(device)\n\n        # 1. Resampling to 22050Hz \n        if sr != self.sample_rate:\n            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate).to(device)\n            waveform = resampler(waveform)\n\n        # 2. Padding or Trimming to a fixed length (critical for batching in DL)\n        if waveform.shape[1] < self.duration_samples:\n            padding_needed = self.duration_samples - waveform.shape[1]\n            waveform = F.pad(waveform, (0, padding_needed))\n        elif waveform.shape[1] > self.duration_samples:\n            waveform = waveform[:, :self.duration_samples]\n\n        if self.transform:\n            features = self.transform(waveform)\n            return features, torch.tensor(label, dtype=torch.long)\n        else:\n            return waveform, torch.tensor(label, dtype=torch.long)\n\n# --- Define Feature Extraction Transform ---\n# MelSpectrogram is a common choice and is GPU-accelerated by torchaudio\nmel_spectrogram_transform = nn.Sequential(torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE,n_fft=N_FFT,hop_length=HOP_LENGTH,n_mels=N_MELS,power=2.0),torchaudio.transforms.AmplitudeToDB(top_db=100.0)).to(device)\n\n# --- Initialize Dataset and DataLoader ---\ndataset = Data_Preprocessing(data_dir=DATA_DIR,sample_rate=SAMPLE_RATE,duration_seconds=DURATION_SECONDS,transform=mel_spectrogram_transform)\n\n\nBATCH_SIZE = 16 # Adjust based on your GPU memory\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n\nlog(f\"Total number of audio files: {len(dataset)}\")\nlog(f\"Number of genres: {len(dataset.label_map)}\")\nlog(f\"Genre map: {dataset.label_map}\")\n\nOUTPUT_SPECTROGRAMS_DIR = 'Spectrogram'\nDPI = 100 # Dots per inch for saved image resolution\nFIGURE_SIZE = (10, 4) # Width, Height in inches, similar to your plt.figure()\n\nfor i in range(len(dataset)):\n    features, label_idx = dataset[i]\n    features_np = features.squeeze(0).cpu().numpy()\n\n    # Get the genre name\n    genre_name = list(dataset.label_map.keys())[label_idx.item()]\n\n    # Create genre-specific subdirectory\n    genre_output_dir = os.path.join(OUTPUT_SPECTROGRAMS_DIR, genre_name)\n    if not os.path.exists(genre_output_dir):\n        os.makedirs(genre_output_dir)\n\n    audio_filename = os.path.basename(dataset.audio_files[i])\n    spectrogram_filename = f\"{os.path.splitext(audio_filename)[0]}.png\"\n\n    save_path = os.path.join(genre_output_dir, spectrogram_filename)\n\n    # Create plot and save\n    plt.figure(figsize=FIGURE_SIZE, dpi=DPI)\n    plt.imshow(features_np, origin='lower', aspect='auto', cmap='magma')\n    plt.title(f\"Genre: {genre_name}\")\n    plt.xlabel(\"Time Frames\")\n    plt.ylabel(\"Mel Bins\")\n    plt.colorbar(format='%+2.0f dB')\n    plt.tight_layout()\n    plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n    plt.close() # Close the plot to free memory\n\nlog(f\"All Mel spectrograms saved to '{OUTPUT_SPECTROGRAMS_DIR}' directory.\")\nshutil.make_archive('spectrograms', 'zip', OUTPUT_SPECTROGRAMS_DIR)","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T06:21:35.086943Z","iopub.execute_input":"2025-06-28T06:21:35.087238Z"}},"outputs":[],"execution_count":null},{"id":"207add39","cell_type":"code","source":"# Define split ratios\ntrain_split = 0.8\nvalidation_split = 0.1\ntest_split = 0.1\n\n# Calculating split sizes\ntotal_size = len(dataset)\ntrain_size = int(train_split * total_size)\nvalidation_size = int(validation_split * total_size)\ntest_size = total_size - train_size - validation_size # Ensure all samples are used\n\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, validation_size, test_size],generator=torch.Generator().manual_seed(42))\n\nlog(f\"Dataset created with sizes: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true},"outputs":[],"execution_count":null},{"id":"2af3029c","cell_type":"code","source":"class AudioCNN_model(nn.Module):\n    def __init__(self, num_classes, n_mels, time_frames):\n        super(AudioCNN_model, self).__init__()\n\n        # Convolutional 1\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n        )\n\n        # Convolutional 2\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n        )\n\n        # Convolutional 3\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n        )\n\n        # Calculate output size after conv layers\n        dummy_input = torch.randn(1, 1, n_mels, time_frames)\n        with torch.no_grad():\n            x = self.conv1(dummy_input)\n            x = self.conv2(x)\n            x = self.conv3(x)\n            self._to_linear = x.shape[1] * x.shape[2] * x.shape[3]\n\n        # Fully connected layers\n        self.fc = nn.Sequential(\n            nn.Linear(self._to_linear, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\ntime_frames = (SAMPLE_RATE * DURATION_SECONDS) // HOP_LENGTH + 1\n\nnum_classes = len(dataset.label_map)\nmodel = AudioCNN_model(num_classes, N_MELS, time_frames).to(device)","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true},"outputs":[],"execution_count":null},{"id":"22c0000e","cell_type":"code","source":"\nLR = 0.001\nEpochs = 20\n\n# Loss function and optmz\ncriterion = nn.CrossEntropyLoss()\noptmz = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n\ndef train_model(model, dataloader, criterion, optmz, device):\n    model.train()\n    running_loss = 0.0\n    correct_predictions = 0\n    total_samples = 0\n\n    for batch_idx, (features, labels) in enumerate(tqdm(dataloader, desc=\"Training\", leave=False)):\n        features = features.to(device)\n        labels = labels.to(device)\n\n        optmz.zero_grad()\n        outputs = model(features)\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        optmz.step()\n\n        running_loss += loss.item() * features.size(0)\n        _, predicted = torch.max(outputs.data, 1)\n        total_samples += labels.size(0)\n        correct_predictions += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / total_samples\n    epoch_accuracy = correct_predictions / total_samples\n    return epoch_loss, epoch_accuracy\n\n\n# Validation Function\ndef evaluate_model(model, dataloader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct_predictions = 0\n    total_samples = 0\n    all_labels = []\n    all_predictions = []\n\n    with torch.no_grad():\n        for features, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n            features = features.to(device)\n            labels = labels.to(device)\n\n            outputs = model(features)\n            loss = criterion(outputs, labels)\n\n            running_loss += loss.item() * features.size(0)\n            _, predicted = torch.max(outputs.data, 1)\n\n            total_samples += labels.size(0)\n            correct_predictions += (predicted == labels).sum().item()\n\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n\n    epoch_loss = running_loss / total_samples\n    epoch_accuracy = correct_predictions / total_samples\n    return epoch_loss, epoch_accuracy, all_labels, all_predictions\n\n\n# Training\nlog(f\"Starting training on {device} for {Epochs} epochs...\")\nbest_val_accuracy = 0.0\n\nfor epoch in range(Epochs):\n    start_time = time.time()\n\n    train_loss, train_accuracy = train_model(model, train_dataloader, criterion, optmz, device)\n    val_loss, val_accuracy, _, _ = evaluate_model(model, val_dataloader, criterion, device)\n\n    end_time = time.time()\n    epoch_duration = end_time - start_time\n\n    log(f\"Epoch {epoch+1}/{Epochs} - Duration: {epoch_duration:.2f}s\")\n    log(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n    log(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n\n    # Save best model based on validation accuracy\n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        torch.save(model.state_dict(), 'best_model.pth')\n        log(f\"  --> Saved best model with Val Acc: {best_val_accuracy:.4f}\")\n\nlog(\"\\nTraining complete!\")\n\n# --- Final Evaluation on Test Set ---\nlog(\"\\nEvaluating on Test Set...\")\nmodel.load_state_dict(torch.load('best_model.pth')) # Load the best model\ntest_loss, test_accuracy, true_labels, predictions = evaluate_model(model, test_dataloader, criterion, device)\n\nlog(f\"\\nTest Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n\n# Detailed Classification Report\nlog(\"\\nClassification Report:\")\ntarget_names = list(dataset.label_map.keys())\nlog(classification_report(true_labels, predictions, target_names=target_names))","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true},"outputs":[],"execution_count":null}]}