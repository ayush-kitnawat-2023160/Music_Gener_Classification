{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1c4cc0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import random_split, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3646b351",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'genres' \n",
    "SAMPLE_RATE = 22050 # GTZAN default sample rate\n",
    "DURATION_SECONDS = 30 # Each audio file is 30 seconds\n",
    "TARGET_SAMPLES = SAMPLE_RATE * DURATION_SECONDS # Total samples per audio file\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Parameters for Mel Spectrogram\n",
    "N_MELS = 128\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "\n",
    "def log(message , file_path=\"log.txt\"):\n",
    "    try:\n",
    "        with open(file_path,'a') as fl:\n",
    "            fl.write(message + '\\n')\n",
    "    except Exception as e:\n",
    "        print(\"File not found\")\n",
    "\n",
    "# --- Custom Dataset Class ---\n",
    "class Data_Preprocessing(Dataset):\n",
    "    def __init__(self, data_dir, sample_rate=22050, duration_seconds=30, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.duration_samples = sample_rate * duration_seconds\n",
    "        self.transform = transform\n",
    "        self.audio_files = []\n",
    "        self.labels = []\n",
    "        self.label_map = {}\n",
    "        self._load_dataset()\n",
    "\n",
    "    def _load_dataset(self):\n",
    "        genres = [d for d in os.listdir(self.data_dir) if os.path.isdir(os.path.join(self.data_dir, d))]\n",
    "        genres.sort()\n",
    "        for i, genre in enumerate(genres):\n",
    "            self.label_map[genre] = i\n",
    "            genre_path = os.path.join(self.data_dir, genre)\n",
    "            for audio_file in os.listdir(genre_path):\n",
    "                if audio_file.endswith('.au'):\n",
    "                    self.audio_files.append(os.path.join(genre_path, audio_file))\n",
    "                    self.labels.append(self.label_map[genre])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load audio (waveform, sample_rate)\n",
    "        # normalized to [-1, 1]\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        waveform = waveform / waveform.abs().max()\n",
    "\n",
    "        # Preprocessing: Resampling, Padding/Trimming, and moving to GPU\n",
    "        waveform = waveform.to(device)\n",
    "\n",
    "        # 1. Resampling to 22050Hz \n",
    "        if sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate).to(device)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # 2. Padding or Trimming to a fixed length (critical for batching in DL)\n",
    "        if waveform.shape[1] < self.duration_samples:\n",
    "            padding_needed = self.duration_samples - waveform.shape[1]\n",
    "            waveform = F.pad(waveform, (0, padding_needed))\n",
    "        elif waveform.shape[1] > self.duration_samples:\n",
    "            waveform = waveform[:, :self.duration_samples]\n",
    "\n",
    "        if self.transform:\n",
    "            features = self.transform(waveform)\n",
    "            return features, torch.tensor(label, dtype=torch.long)\n",
    "        else:\n",
    "            return waveform, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# --- Define Feature Extraction Transform ---\n",
    "# MelSpectrogram is a common choice and is GPU-accelerated by torchaudio\n",
    "mel_spectrogram_transform = nn.Sequential(torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE,n_fft=N_FFT,hop_length=HOP_LENGTH,n_mels=N_MELS,power=2.0),torchaudio.transforms.AmplitudeToDB(top_db=100.0)).to(device)\n",
    "\n",
    "# --- Initialize Dataset and DataLoader ---\n",
    "dataset = Data_Preprocessing(data_dir=DATA_DIR,sample_rate=SAMPLE_RATE,duration_seconds=DURATION_SECONDS,transform=mel_spectrogram_transform)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16 # Adjust based on your GPU memory\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "log(f\"Total number of audio files: {len(dataset)}\")\n",
    "log(f\"Number of genres: {len(dataset.label_map)}\")\n",
    "log(f\"Genre map: {dataset.label_map}\")\n",
    "\n",
    "OUTPUT_SPECTROGRAMS_DIR = 'Spectrogram'\n",
    "DPI = 100 # Dots per inch for saved image resolution\n",
    "FIGURE_SIZE = (10, 4) # Width, Height in inches, similar to your plt.figure()\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    features, label_idx = dataset[i]\n",
    "    features_np = features.squeeze(0).cpu().numpy()\n",
    "\n",
    "    # Get the genre name\n",
    "    genre_name = list(dataset.label_map.keys())[label_idx.item()]\n",
    "\n",
    "    # Create genre-specific subdirectory\n",
    "    genre_output_dir = os.path.join(OUTPUT_SPECTROGRAMS_DIR, genre_name)\n",
    "    if not os.path.exists(genre_output_dir):\n",
    "        os.makedirs(genre_output_dir)\n",
    "\n",
    "    audio_filename = os.path.basename(dataset.data_info[i]['filepath'])\n",
    "    spectrogram_filename = f\"{os.path.splitext(audio_filename)[0]}.png\"\n",
    "\n",
    "    save_path = os.path.join(genre_output_dir, spectrogram_filename)\n",
    "\n",
    "    # Create plot and save\n",
    "    plt.figure(figsize=FIGURE_SIZE, dpi=DPI)\n",
    "    plt.imshow(features_np, origin='lower', aspect='auto', cmap='magma')\n",
    "    plt.title(f\"Genre: {genre_name}\")\n",
    "    plt.xlabel(\"Time Frames\")\n",
    "    plt.ylabel(\"Mel Bins\")\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close() # Close the plot to free memory\n",
    "\n",
    "log(f\"All Mel spectrograms saved to '{OUTPUT_SPECTROGRAMS_DIR}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207add39",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define split ratios\n",
    "train_split = 0.8\n",
    "validation_split = 0.1\n",
    "test_split = 0.1\n",
    "\n",
    "# Calculating split sizes\n",
    "total_size = len(dataset)\n",
    "train_size = int(train_split * total_size)\n",
    "validation_size = int(validation_split * total_size)\n",
    "test_size = total_size - train_size - validation_size # Ensure all samples are used\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, validation_size, test_size],generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "log(f\"Dataset created with sizes: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af3029c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class AudioCNN_model(nn.Module):\n",
    "    def __init__(self, num_classes, n_mels, time_frames):\n",
    "        super(AudioCNN_model, self).__init__()\n",
    "\n",
    "        # Convolutional 1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        )\n",
    "\n",
    "        # Convolutional 2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        )\n",
    "\n",
    "        # Convolutional 3\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        )\n",
    "\n",
    "        # Calculate output size after conv layers\n",
    "        dummy_input = torch.randn(1, 1, n_mels, time_frames)\n",
    "        with torch.no_grad():\n",
    "            x = self.conv1(dummy_input)\n",
    "            x = self.conv2(x)\n",
    "            x = self.conv3(x)\n",
    "            self._to_linear = x.shape[1] * x.shape[2] * x.shape[3]\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self._to_linear, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "time_frames = (SAMPLE_RATE * DURATION_SECONDS) // HOP_LENGTH + 1\n",
    "\n",
    "num_classes = len(dataset.label_map)\n",
    "model = AudioCNN_model(num_classes, N_MELS, time_frames).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c0000e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "LR = 0.001\n",
    "Epochs = 20\n",
    "\n",
    "# Loss function and optmz\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optmz = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "\n",
    "def train_model(model, dataloader, criterion, optmz, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (features, labels) in enumerate(tqdm(dataloader, desc=\"Training\", leave=False)):\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optmz.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optmz.step()\n",
    "\n",
    "        running_loss += loss.item() * features.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_accuracy = correct_predictions / total_samples\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "\n",
    "# Validation Function\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * features.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_accuracy = correct_predictions / total_samples\n",
    "    return epoch_loss, epoch_accuracy, all_labels, all_predictions\n",
    "\n",
    "\n",
    "# Training\n",
    "log(f\"Starting training on {device} for {Epochs} epochs...\")\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_accuracy = train_model(model, train_dataloader, criterion, optmz, device)\n",
    "    val_loss, val_accuracy, _, _ = evaluate_model(model, val_dataloader, criterion, device)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "\n",
    "    log(f\"Epoch {epoch+1}/{Epochs} - Duration: {epoch_duration:.2f}s\")\n",
    "    log(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "    log(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Save best model based on validation accuracy\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        log(f\"  --> Saved best model with Val Acc: {best_val_accuracy:.4f}\")\n",
    "\n",
    "log(\"\\nTraining complete!\")\n",
    "\n",
    "# --- Final Evaluation on Test Set ---\n",
    "log(\"\\nEvaluating on Test Set...\")\n",
    "model.load_state_dict(torch.load('best_model.pth')) # Load the best model\n",
    "test_loss, test_accuracy, true_labels, predictions = evaluate_model(model, test_dataloader, criterion, device)\n",
    "\n",
    "log(f\"\\nTest Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Detailed Classification Report\n",
    "log(\"\\nClassification Report:\")\n",
    "target_names = list(dataset.label_map.keys())\n",
    "log(classification_report(true_labels, predictions, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
